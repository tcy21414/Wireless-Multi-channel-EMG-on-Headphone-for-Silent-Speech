## Import Necessary Libraries

import os
import glob
import numpy as np
import pandas as pd
import random

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from scipy.signal import butter, filtfilt
from sklearn.model_selection import train_test_split

## Filter Definition

def bandpass_filter(signal, fs=1000, lowcut=20, highcut=450, order=4):
    """
    Perform (lowcut, highcut) bandpass filtering on a 1D numpy array.
    - signal: A 1D signal with shape (N,)
    - fs: Sampling rate (default 1000 Hz)
    - lowcut, highcut: Cutoff frequencies for the bandpass filter
    - order: The order of the filter
    
    Returns the filtered signal, maintaining the original length.
    """
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    filtered = filtfilt(b, a, signal, axis=0)
    return filtered


## Data Augmentation Definition

def random_time_shift(signal, max_shift=100):
    """
    Apply random time shifts to the signal. A random shift value is chosen from 
    [-max_shift, +max_shift], and the signal is shifted using padding/truncation.
    signal: shape [channels, time]
    """
    channels, time_len = signal.shape
    shift = np.random.randint(-max_shift, max_shift+1)
    if shift == 0:
        return signal
    
    # Create an empty array for padding
    shifted = np.zeros((channels, time_len), dtype=signal.dtype)
    if shift > 0:
        # Shift to the right by 'shift', fill the first 'shift' points with zeros
        # and copy the remaining signal.
        shifted[:, shift:] = signal[:, :time_len-shift]
    else:
        # Shift to the left by abs(shift), leaving the end empty.
        shift_abs = abs(shift)
        shifted[:, :time_len-shift_abs] = signal[:, shift_abs:]
    return shifted

def random_noise(signal, noise_level=0.01):
    """
    Add Gaussian noise to the signal.
    noise_level: Noise level as a factor of the signal's standard deviation.
    """
    std = np.std(signal)
    if std < 1e-6:
        std = 1.0  # Prevent issues with signals having zero variance.
    noise = np.random.randn(*signal.shape) * (std * noise_level)
    return signal + noise

def random_scale_offset(signal, scale_range=(0.9, 1.1), offset_range=(-0.1, 0.1)):
    """
    Apply random scaling and offset to the signal.
    scale_range: Range for the scaling factor (min, max)
    offset_range: Range for the offset (min, max)
    """
    scale = np.random.uniform(scale_range[0], scale_range[1])
    offset = np.random.uniform(offset_range[0], offset_range[1])
    return signal * scale + offset


## Dataset Loading Definition

def load_earssi_dataset(csv_folder):
    """
    Load and combine data from all CSV files in the specified folder.
    Each CSV corresponds to 50 repetitions of a single word class.
    Returns:
        data_list: list of ndarray, each element has shape [4, 3000]
        label_list: list of int, with values ranging from 1 to 10.
    """
    all_data = []
    all_labels = []

    # Find all CSV file paths
    csv_files = sorted(glob.glob(os.path.join(csv_folder, '*.csv')))
    print(f"Found {len(csv_files)} CSV files in {csv_folder}.")

    for csv_file in csv_files:
        df = pd.read_csv(csv_file, sep=None, engine='python')  # Handles different delimiters like ',' or '\t'.

        # Ensure column names match the example: ['sample_id', 'time_index', 'ch1', 'ch2', 'ch3', 'ch4', 'label']
        # If the delimiter is not '\t', adjust with `delimiter=','` or other appropriate arguments.

        # Group by sample_id
        grouped = df.groupby('sample_id')
        for sid, group in grouped:
            # 'group' contains all rows for the current sample_id, with shape ~ (3000, 7).
            # Extract columns ch1~ch4
            ch1 = group['ch1'].values.astype(np.float32)
            ch2 = group['ch2'].values.astype(np.float32)
            ch3 = group['ch3'].values.astype(np.float32)
            ch4 = group['ch4'].values.astype(np.float32)
            
            # Apply bandpass filtering
            ch1_f = bandpass_filter(ch1, fs=1000, lowcut=20, highcut=450, order=4)
            ch2_f = bandpass_filter(ch2, fs=1000, lowcut=20, highcut=450, order=4)
            ch3_f = bandpass_filter(ch3, fs=1000, lowcut=20, highcut=450, order=4)
            ch4_f = bandpass_filter(ch4, fs=1000, lowcut=20, highcut=450, order=4)
            
            # Combine into [4, 3000]
            # Note: The typical format is (channels, time steps) for ease of use in 1D Conv layers.
            data_4ch = np.stack([ch1_f, ch2_f, ch3_f, ch4_f], axis=0)  # [4, 3000]
            
            # Extract label (all rows under the same sample_id have the same label)
            label_vals = group['label'].unique()
            if len(label_vals) != 1:
                raise ValueError(f"Multiple labels found under sample_id={sid}. Please check the data!")
            label = label_vals[0]
            
            all_data.append(data_4ch.astype(np.float32))
            all_labels.append(int(label))

    all_data = np.array(all_data)   # shape [N, 4, 3000]
    all_labels = np.array(all_labels)  # shape [N, ]
    print(f"Total samples loaded: {all_data.shape[0]}")
    return all_data, all_labels


## Dataset Initializing Definition

class EarSSIDataset(Dataset):
    def __init__(self, data_array, label_array, do_augment=False):
        """
        Initializes the dataset.
        - data_array: A numpy array of shape [N, 4, 3000], containing the data samples.
        - label_array: A numpy array of shape [N], containing the labels.
        - do_augment: A boolean flag indicating whether to apply data augmentation.
        """
        # Force data_array to be float32
        self.data = data_array.astype(np.float32)
        # Convert labels to int64 and make them zero-based (label - 1)
        self.labels = (label_array - 1).astype(np.int64)
        self.do_augment = do_augment
    
    def __len__(self):
        """
        Returns the total number of samples in the dataset.
        """
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        Returns a single sample (x, y) from the dataset.
        - x: The data sample, shape [4, 3000], as a torch.Tensor of dtype float32.
        - y: The label, as a torch.Tensor of dtype long.
        """
        x = self.data[idx]   # shape [4, 3000], float32
        y = self.labels[idx] # scalar
        
        # Apply data augmentation if enabled
        if self.do_augment:
            x = self.augment_sample(x)
        
        # Convert to torch.Tensor
        x_t = torch.from_numpy(x)
        y_t = torch.tensor(y, dtype=torch.long)
        
        return x_t, y_t
    
    def augment_sample(self, signal):
        """
        Applies random data augmentation to a single sample signal of shape (4, T).
        Augmentation steps include:
        1) Random time shift
        2) Random Gaussian noise addition
        3) Random scaling and offset
        """
        # 1) Random time shift
        if random.random() < 0.5:
            signal = random_time_shift(signal, max_shift=100)
    
        # 2) Random noise addition
        if random.random() < 0.5:
            signal = random_noise(signal, noise_level=0.02)  # Example: noise level 0.02
    
        # 3) Random scaling and offset
        if random.random() < 0.5:
            signal = random_scale_offset(signal, scale_range=(0.9, 1.1), offset_range=(-0.1, 0.1))
    
        # Force the result back to float32
        signal = signal.astype(np.float32)
        return signal


## Neural Network Backbone Definition

class SEBlock1D(nn.Module):
    def __init__(self, channels, reduction=8):
        """
        Squeeze-and-Excitation block for 1D inputs.
        - channels: Number of input channels.
        - reduction: Reduction ratio for the fully connected layers.
        """
        super(SEBlock1D, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        """
        Forward pass of SEBlock.
        - x: Input tensor of shape [B, C, T].
        Returns:
        - Scaled input tensor of shape [B, C, T].
        """
        b, c, t = x.size()
        y = self.avg_pool(x)    # [B, C, 1]
        y = y.view(b, c)        # [B, C]
        y = self.fc(y)          # [B, C], scaled between 0 and 1.
        y = y.view(b, c, 1)
        return x * y

class ResidualBlock1D(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, 
                 downsample=None, reduction=8, p_dropout=0.0):
        """
        A 1D residual block with SEBlock.
        - in_channels: Number of input channels.
        - out_channels: Number of output channels.
        - kernel_size: Size of the convolutional kernel.
        - stride: Stride for the first convolutional layer.
        - downsample: Downsampling layer for matching dimensions.
        - reduction: Reduction ratio for the SEBlock.
        - p_dropout: Dropout probability.
        """
        super(ResidualBlock1D, self).__init__()
        padding = (kernel_size - 1) // 2

        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride,
                               padding=padding, bias=False)
        self.bn1 = nn.BatchNorm1d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1,
                               padding=padding, bias=False)
        self.bn2 = nn.BatchNorm1d(out_channels)
        
        self.se = SEBlock1D(out_channels, reduction=reduction)
        self.downsample = downsample
        
        self.dropout = nn.Dropout(p=p_dropout)

    def forward(self, x):
        """
        Forward pass of ResidualBlock1D.
        - x: Input tensor of shape [B, C_in, T].
        Returns:
        - Output tensor of shape [B, C_out, T].
        """
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        
        out = self.se(out)
        
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity
        out = self.relu(out)
        
        out = self.dropout(out)
        
        return out

class SEResNet1D(nn.Module):
    def __init__(self, num_classes=10, p_dropout=0.5):
        """
        Squeeze-and-Excitation ResNet for 1D inputs.
        - num_classes: Number of output classes.
        - p_dropout: Dropout probability applied to each residual block and before the final fully connected layer.
        """
        super(SEResNet1D, self).__init__()
        
        self.conv1 = nn.Conv1d(4, 16, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm1d(16)
        self.relu = nn.ReLU(inplace=True)
        self.pool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)
        
        self.layer1 = self._make_layer(16, 16, num_blocks=2, stride=1, p_dropout=p_dropout)
        self.layer2 = self._make_layer(16, 32, num_blocks=2, stride=2, p_dropout=p_dropout)
        self.layer3 = self._make_layer(32, 64, num_blocks=2, stride=2, p_dropout=p_dropout)
        
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        self.fc_dropout = nn.Dropout(p=p_dropout)
        self.fc = nn.Linear(64, num_classes)
    
    def _make_layer(self, in_channels, out_channels, num_blocks, stride, p_dropout):
        """
        Creates a layer of residual blocks.
        - in_channels: Number of input channels for the first block.
        - out_channels: Number of output channels for all blocks.
        - num_blocks: Number of residual blocks in the layer.
        - stride: Stride for the first block in the layer.
        - p_dropout: Dropout probability for the blocks.
        """
        downsample = None
        if (stride != 1) or (in_channels != out_channels):
            downsample = nn.Sequential(
                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm1d(out_channels)
            )
        layers = []
        layers.append(ResidualBlock1D(in_channels, out_channels, stride=stride,
                                      downsample=downsample, p_dropout=p_dropout))
        for _ in range(1, num_blocks):
            layers.append(ResidualBlock1D(out_channels, out_channels, p_dropout=p_dropout))
        return nn.Sequential(*layers)
    
    def forward(self, x):
        """
        Forward pass of SEResNet1D.
        - x: Input tensor of shape [B, 4, T].
        Returns:
        - logits: Output tensor of shape [B, num_classes].
        """
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.pool(out)
        
        out = self.layer1(out)
        out = self.layer2(out)
        out = self.layer3(out)
        
        out = self.global_pool(out)   # [B, 64, 1]
        out = out.view(out.size(0), -1)  # [B, 64]
        
        out = self.fc_dropout(out)
        logits = self.fc(out)         # [B, num_classes]
        return logits


## Training Loop Definition

def train_one_epoch(model, loader, optimizer, criterion, device='cpu'):
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    
    for batch_x, batch_y in loader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)
        
        optimizer.zero_grad()
        logits = model(batch_x)
        loss = criterion(logits, batch_y)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * batch_x.size(0)
        preds = torch.argmax(logits, dim=1)
        correct += (preds == batch_y).sum().item()
        total += batch_x.size(0)
    
    avg_loss = total_loss / total
    acc = correct / total
    return avg_loss, acc

def eval_model(model, loader, criterion, device='cpu'):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_x, batch_y in loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            logits = model(batch_x)
            loss = criterion(logits, batch_y)
            
            total_loss += loss.item() * batch_x.size(0)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == batch_y).sum().item()
            total += batch_x.size(0)
    
    avg_loss = total_loss / total
    acc = correct / total
    return avg_loss, acc


## Main Loop

def main():
    # 1) Load the data
    csv_folder = "/Users/chenyutang/Ear_SSI_Dataset/CSV" ## The dataset document
    data_array, label_array = load_earssi_dataset(csv_folder)
    print("Dataset shape:", data_array.shape, "Labels shape:", label_array.shape)
    # data_array: [N, 4, 3000], float32
    # label_array: [N], int in [1..10]
    
    # 2) Split the data into training and testing sets
    train_data, test_data, train_label, test_label = train_test_split(
        data_array, label_array, test_size=0.2, random_state=42, shuffle=True
    )
    
    # 3) Create Dataset objects, enabling `do_augment=True` for the training set
    train_dataset = EarSSIDataset(train_data, train_label, do_augment=True)
    test_dataset  = EarSSIDataset(test_data, test_label, do_augment=False)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True)
    test_loader  = DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    # 4) Define the model, loss function, and optimizer (with weight_decay)
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")  
    # For CUDA: device = torch.device("cuda") ...
    
    model = SEResNet1D(num_classes=10, p_dropout=0.4).to(device)  # p_dropout=0.3 as an example
    
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)
    # weight_decay=1e-4 is the L2 regularization strength, can be adjusted as needed
    
    # 5) Train the model
    epochs = 70
    best_val_acc = 0.0
    for epoch in range(1, epochs+1):
        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)
        val_loss, val_acc = eval_model(model, test_loader, criterion, device)
        
        print(f"Epoch [{epoch}/{epochs}] "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
        
        # Optional: Early stopping or saving the best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_model.pt")
            print(f"  [*] New best model saved (Val Acc={best_val_acc:.4f})")
    
    print("Training finished. Best validation accuracy:", best_val_acc)

if __name__ == "__main__":
    main()

